# Multimodel Chatbot

A powerful conversational AI system featuring Retrieval-Augmented Generation (RAG) with support for multiple language models including OpenAI GPT and local Hugging Face models.

## ğŸŒŸ Features

- ğŸ¤– **Multiple Model Support** â€” Switch between OpenAI GPT and local Hugging Face models
- ğŸ” **Retrieval-Augmented Generation (RAG)** â€” Context-aware responses using your documents
- ğŸ’¬ **Conversation Memory** â€” Multi-turn conversations with context retention
- ğŸ“„ **Document Processing** â€” Support for PDF, TXT, and other document formats
- ğŸ¨ **Modern Web Interface** â€” Built with Gradio for intuitive user experience
- ğŸ” **Privacy First** â€” Option to run completely locally with open-source models
- âš¡ **Fast & Efficient** â€” Optimized vector search with FAISS

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- 8GB RAM minimum (16GB recommended for local models)
- OpenAI API key (optional, for GPT models)

### Installation

```bash
# Clone the repository
git clone https://github.com/AshminDhungana/multimodel-chatbot.git
cd multimodel-chatbot

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Edit .env and add your configuration
```

### Configuration

Edit the `.env` file with your settings:

```env
# Model Configuration
OPENAI_API_KEY=your_api_key_here  # Optional: Leave empty for local-only mode
MODEL_TYPE=hybrid                  # Options: hybrid, openai, local
DEFAULT_MODEL=gpt-3.5-turbo       # For OpenAI models

# RAG Configuration
CHUNK_SIZE=500
CHUNK_OVERLAP=100
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Generation Parameters
TEMPERATURE=0.7
MAX_TOKENS=2048
TOP_K=5                           # Number of retrieved documents
```

### Usage

```bash
# Start the application
python app.py
```

Then open your browser and navigate to `http://localhost:7860`

## ğŸ“ Project Structure

```
multimodel-chatbot/
â”œâ”€â”€ app.py                      # Main Gradio application
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_pipeline.py        # RAG logic and orchestration
â”‚   â”œâ”€â”€ embeddings.py          # Embedding models manager
â”‚   â”œâ”€â”€ llm_models.py          # LLM interface (OpenAI + HF)
â”‚   â”œâ”€â”€ document_loader.py     # Document processing
â”‚   â””â”€â”€ vectorstore.py         # Vector database operations
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_documents/      # Sample files for testing
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_rag.py           # RAG pipeline tests
â”‚   â”œâ”€â”€ test_models.py        # Model integration tests
â”‚   â””â”€â”€ test_embeddings.py    # Embedding tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ QUICKSTART.md
â”‚   â”œâ”€â”€ SETUP_GUIDE.md
â”‚   
â”œâ”€â”€ requirements.txt          #  dependencies
â”œâ”€â”€ .env                     # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
```

## ğŸ’» Technology Stack

- **[LangChain](https://langchain.com/)** â€” LLM orchestration and chains
- **[Gradio](https://gradio.app/)** â€” Web UI framework
- **[Hugging Face](https://huggingface.co/)** â€” Embeddings and local models
- **[OpenAI](https://openai.com/)** â€” GPT language models
- **[FAISS](https://github.com/facebookresearch/faiss)** â€” Vector similarity search
- **[Sentence Transformers](https://www.sbert.net/)** â€” Text embeddings

## ğŸ¯ Use Cases

- **Document Q&A** â€” Ask questions about your documents
- **Knowledge Base Search** â€” Retrieve relevant information from large corpora
- **Research Assistant** â€” Get AI-powered insights from your research papers
- **Customer Support** â€” Build context-aware chatbots for support
- **Educational Tool** â€” Learn and explore topics with AI assistance

## ğŸ”§ Advanced Configuration

### Using Local Models Only

Set `MODEL_TYPE=local` in your `.env` file to run completely offline:

```env
MODEL_TYPE=local
LOCAL_LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

### Custom Document Sources

Add your documents to the `data/` directory or configure custom paths:

```python
from src.document_loader import DocumentLoader

loader = DocumentLoader()
documents = loader.load_directory("path/to/your/documents")
```

### Model Selection

Switch between models in the UI or programmatically:

```python
from src.llm_models import LLMManager

llm = LLMManager()
llm.set_model("gpt-4")  # or "mistral-7b", "llama-2-7b", etc.
```

## ğŸ“Š Performance

- **Response Time:** ~2-5 seconds (with OpenAI)
- **Local Mode:** ~5-15 seconds (depends on hardware)
- **Document Processing:** ~1-2 seconds per MB
- **Memory Usage:** 2-4GB (local models require 8-16GB)


## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- OpenAI for GPT models
- Hugging Face for open-source models and infrastructure
- LangChain community for excellent documentation
- All contributors and supporters

## ğŸ“§ Contact

- **GitHub Issues:** [Report bugs or request features](https://github.com/AshminDhungana/multimodel-chatbot/issues)
- **Discussions:** [Join the conversation](https://github.com/AshminDhungana/multimodel-chatbot/discussions)


**Built with â¤ï¸ for the AI community**